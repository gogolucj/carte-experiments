{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Graph + BERT 기반 영화 임베딩 생성\n",
    "\n",
    "기존 KG GNN에 BERT 텍스트 임베딩(tagline, overview)을 추가한 버전입니다.\n",
    "\n",
    "**하이브리드 방식:**\n",
    "- 영화 노드: BERT(tagline + overview) 임베딩 사용\n",
    "- 엔티티 노드: fastText 임베딩 (기존 방식 유지)\n",
    "- 그래프 구조: 영화-엔티티 Knowledge Graph\n",
    "\n",
    "**vs kg_gnn (기존):**\n",
    "- kg_gnn: 영화 노드를 release_year로만 초기화\n",
    "- kg_gnn_bert: 영화 노드를 BERT(tagline+overview)로 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/Users/jisoo/projects/thesis/carte_test\")\n",
    "\n",
    "from config import PROCESSED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import HeteroConv, SAGEConv\n",
    "from huggingface_hub import hf_hub_download\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "import fasttext\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영화 카탈로그 로드\n",
    "catalog = pd.read_parquet(PROCESSED.MOVIE_CATALOG_PARQUET)\n",
    "ratings = pd.read_parquet(PROCESSED.RATINGS_PARQUET)\n",
    "\n",
    "# ratings에 있는 영화만 필터링\n",
    "movie_ids_in_ratings = ratings['movieId'].unique()\n",
    "catalog_filtered = catalog[catalog['movieId'].isin(movie_ids_in_ratings)].reset_index(drop=True)\n",
    "\n",
    "print(f\"영화 수: {len(catalog_filtered):,}\")\n",
    "catalog_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BERT 임베딩 생성 (tagline + overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_text_fields(row, text_cols):\n",
    "    \"\"\"여러 텍스트 컬럼을 하나의 문자열로 결합\"\"\"\n",
    "    parts = []\n",
    "    for col in text_cols:\n",
    "        val = row.get(col)\n",
    "        if pd.notna(val) and str(val).strip():\n",
    "            parts.append(str(val).strip())\n",
    "    return \" \".join(parts) if parts else \"\"\n",
    "\n",
    "\n",
    "def compute_bert_embeddings(df, text_cols, model_name, batch_size=64, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    BERT(sentence-transformers)로 텍스트 임베딩 생성\n",
    "    \"\"\"\n",
    "    # 텍스트 결합\n",
    "    texts = df.apply(lambda row: combine_text_fields(row, text_cols), axis=1).tolist()\n",
    "    \n",
    "    # 빈 텍스트 처리\n",
    "    texts = [t if t else \"[empty]\" for t in texts]\n",
    "    \n",
    "    non_empty = sum(1 for t in texts if t != \"[empty]\")\n",
    "    print(f\"[BERT] Non-empty texts: {non_empty:,} / {len(texts):,}\")\n",
    "    print(f\"[BERT] Sample text: {texts[0][:200]}...\")\n",
    "    \n",
    "    # 모델 로드\n",
    "    model = SentenceTransformer(model_name, device=device)\n",
    "    print(f\"[BERT] Model: {model_name}\")\n",
    "    print(f\"[BERT] Embedding dim: {model.get_sentence_embedding_dimension()}\")\n",
    "    \n",
    "    # 임베딩 생성\n",
    "    embeddings = model.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "    )\n",
    "    \n",
    "    print(f\"[BERT] Output shape: {embeddings.shape}\")\n",
    "    return embeddings.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT 임베딩 생성\n",
    "BERT_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # 384차원\n",
    "TEXT_COLS = [\"tagline\", \"overview\"]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\\n\")\n",
    "\n",
    "bert_embeddings = compute_bert_embeddings(\n",
    "    catalog_filtered,\n",
    "    TEXT_COLS,\n",
    "    model_name=BERT_MODEL,\n",
    "    batch_size=64,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT 임베딩을 300차원으로 projection\n",
    "class LinearProjection(nn.Module):\n",
    "    \"\"\"BERT 임베딩을 300차원으로 projection\"\"\"\n",
    "    def __init__(self, input_dim, output_dim=300):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(input_dim, output_dim),\n",
    "            nn.LayerNorm(output_dim),\n",
    "        )\n",
    "        nn.init.xavier_uniform_(self.proj[0].weight)\n",
    "        nn.init.zeros_(self.proj[0].bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.proj(x)\n",
    "\n",
    "\n",
    "def project_bert_to_300(bert_emb, device=\"cpu\"):\n",
    "    \"\"\"BERT 임베딩을 300차원으로 projection\"\"\"\n",
    "    input_dim = bert_emb.shape[1]\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    proj = LinearProjection(input_dim, 300).to(device)\n",
    "    proj.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x = torch.from_numpy(bert_emb).to(device)\n",
    "        out = proj(x)\n",
    "        return out.cpu().numpy().astype(np.float32)\n",
    "\n",
    "\n",
    "bert_300 = project_bert_to_300(bert_embeddings, device=device)\n",
    "print(f\"Projected shape: {bert_300.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Knowledge Graph 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엔티티 타입 정의\n",
    "ENTITY_TYPES = {\n",
    "    'actor': ['actor_1', 'actor_2', 'actor_3'],\n",
    "    'director': ['director_1'],\n",
    "    'writer': ['writer_1'],\n",
    "    'genre': ['genre_1'],\n",
    "    'company': ['produced_by_company_1'],\n",
    "    'country': ['produced_in_country_1'],\n",
    "    'language': ['spoken_language_1'],\n",
    "}\n",
    "\n",
    "def build_knowledge_graph(df, movie_bert_emb):\n",
    "    \"\"\"영화 카탈로그에서 Heterogeneous Knowledge Graph 구축 (BERT 임베딩 포함)\"\"\"\n",
    "    \n",
    "    # 영화 ID 매핑\n",
    "    movie_ids = df['movieId'].tolist()\n",
    "    movie_to_idx = {mid: i for i, mid in enumerate(movie_ids)}\n",
    "    \n",
    "    # 엔티티별 유니크 값 수집 및 매핑\n",
    "    entity_to_idx = {}\n",
    "    for etype, cols in ENTITY_TYPES.items():\n",
    "        unique_vals = set()\n",
    "        for col in cols:\n",
    "            if col in df.columns:\n",
    "                unique_vals.update(df[col].dropna().unique())\n",
    "        entity_to_idx[etype] = {v: i for i, v in enumerate(sorted(unique_vals))}\n",
    "    \n",
    "    # 엣지 구축\n",
    "    edges = {etype: ([], []) for etype in ENTITY_TYPES}\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Building edges\"):\n",
    "        movie_idx = movie_to_idx[row['movieId']]\n",
    "        \n",
    "        for etype, cols in ENTITY_TYPES.items():\n",
    "            for col in cols:\n",
    "                if col in df.columns and pd.notna(row[col]):\n",
    "                    entity_val = row[col]\n",
    "                    if entity_val in entity_to_idx[etype]:\n",
    "                        entity_idx = entity_to_idx[etype][entity_val]\n",
    "                        edges[etype][0].append(movie_idx)\n",
    "                        edges[etype][1].append(entity_idx)\n",
    "    \n",
    "    # HeteroData 구축\n",
    "    data = HeteroData()\n",
    "    \n",
    "    # 영화 노드: BERT 임베딩 사용\n",
    "    data['movie'].x = torch.from_numpy(movie_bert_emb)\n",
    "    data['movie'].num_nodes = len(movie_ids)\n",
    "    \n",
    "    # 엔티티 노드 수 설정\n",
    "    for etype, mapping in entity_to_idx.items():\n",
    "        data[etype].num_nodes = len(mapping)\n",
    "    \n",
    "    # 엣지 설정 (양방향)\n",
    "    for etype, (src, dst) in edges.items():\n",
    "        if len(src) > 0:\n",
    "            edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
    "            data['movie', f'has_{etype}', etype].edge_index = edge_index\n",
    "            data[etype, f'has_{etype}_rev', 'movie'].edge_index = edge_index.flip(0)\n",
    "    \n",
    "    return data, movie_to_idx, entity_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KG 구축 (BERT 임베딩 포함)\n",
    "kg_data, movie_to_idx, entity_to_idx = build_knowledge_graph(catalog_filtered, bert_300)\n",
    "\n",
    "print(\"\\n[Knowledge Graph 통계]\")\n",
    "print(f\"  영화 노드: {kg_data['movie'].num_nodes:,} (BERT 초기화)\")\n",
    "for etype in entity_to_idx:\n",
    "    print(f\"  {etype} 노드: {kg_data[etype].num_nodes:,}\")\n",
    "\n",
    "total_edges = sum(kg_data[et].edge_index.size(1) for et in kg_data.edge_types)\n",
    "print(f\"\\n  총 엣지 수: {total_edges:,}\")\n",
    "print(f\"  엣지 타입: {len(kg_data.edge_types)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 엔티티 노드 초기화 (fastText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastText 모델 로드\n",
    "print(\"fastText 모델 로딩...\")\n",
    "ft_path = hf_hub_download(repo_id=\"hi-paris/fastText\", filename=\"cc.en.300.bin\")\n",
    "ft_model = fasttext.load_model(ft_path)\n",
    "print(\"완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_embeddings(mapping, ft_model, dim=300):\n",
    "    \"\"\"엔티티 이름을 fastText로 임베딩\"\"\"\n",
    "    idx_to_entity = {v: k for k, v in mapping.items()}\n",
    "    n = len(mapping)\n",
    "    emb = np.zeros((n, dim), dtype=np.float32)\n",
    "    \n",
    "    for idx in range(n):\n",
    "        entity_name = idx_to_entity[idx]\n",
    "        emb[idx] = ft_model.get_sentence_vector(str(entity_name))\n",
    "    \n",
    "    return torch.from_numpy(emb)\n",
    "\n",
    "# 각 엔티티 타입별 초기 임베딩\n",
    "print(\"엔티티 임베딩 생성...\")\n",
    "for etype, mapping in entity_to_idx.items():\n",
    "    kg_data[etype].x = get_entity_embeddings(mapping, ft_model)\n",
    "    print(f\"  {etype}: {kg_data[etype].x.shape}\")\n",
    "\n",
    "print(f\"  movie: {kg_data['movie'].x.shape} (BERT)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Heterogeneous GNN 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNN(nn.Module):\n",
    "    \"\"\"Heterogeneous Graph Neural Network using HeteroConv\"\"\"\n",
    "    \n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, edge_types, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            in_channels = in_dim if i == 0 else hidden_dim\n",
    "            out_channels = out_dim if i == num_layers - 1 else hidden_dim\n",
    "            \n",
    "            conv_dict = {}\n",
    "            for edge_type in edge_types:\n",
    "                conv_dict[edge_type] = SAGEConv(in_channels, out_channels)\n",
    "            self.convs.append(HeteroConv(conv_dict, aggr='mean'))\n",
    "    \n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "            if i < self.num_layers - 1:\n",
    "                x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n",
    "        return x_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "model = HeteroGNN(\n",
    "    in_dim=300,\n",
    "    hidden_dim=300,\n",
    "    out_dim=300,\n",
    "    edge_types=kg_data.edge_types,\n",
    "    num_layers=2\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\n모델 파라미터: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 학습 (Link Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kg_embeddings(model, data, epochs=100, lr=0.01):\n",
    "    \"\"\"Link prediction 기반 unsupervised 학습\"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # 학습할 엣지 타입 (movie -> entity)\n",
    "    train_edge_types = [et for et in data.edge_types if et[0] == 'movie']\n",
    "    \n",
    "    losses = []\n",
    "    pbar = tqdm(range(epochs), desc=\"Training\")\n",
    "    \n",
    "    for epoch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        out_dict = model(data.x_dict, data.edge_index_dict)\n",
    "        \n",
    "        total_loss = 0\n",
    "        for edge_type in train_edge_types:\n",
    "            src_type, _, dst_type = edge_type\n",
    "            edge_index = data[edge_type].edge_index\n",
    "            \n",
    "            if edge_index.size(1) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Positive samples\n",
    "            src_emb = out_dict[src_type][edge_index[0]]\n",
    "            dst_emb = out_dict[dst_type][edge_index[1]]\n",
    "            pos_score = (src_emb * dst_emb).sum(dim=1)\n",
    "            \n",
    "            # Negative samples\n",
    "            neg_dst_idx = torch.randint(0, data[dst_type].num_nodes, (edge_index.size(1),), device=data[dst_type].x.device)\n",
    "            neg_dst_emb = out_dict[dst_type][neg_dst_idx]\n",
    "            neg_score = (src_emb * neg_dst_emb).sum(dim=1)\n",
    "            \n",
    "            # BPR Loss\n",
    "            loss = -F.logsigmoid(pos_score - neg_score).mean()\n",
    "            total_loss += loss\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(total_loss.item())\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{total_loss.item():.2f}'})\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 device로 이동\n",
    "kg_data = kg_data.to(device)\n",
    "\n",
    "# 학습 실행\n",
    "losses = train_kg_embeddings(model, kg_data, epochs=100, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 손실 시각화\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Knowledge Graph + BERT Embedding Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"최종 Loss: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 임베딩 추출 및 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 추출\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out_dict = model(kg_data.x_dict, kg_data.edge_index_dict)\n",
    "    movie_embeddings = out_dict['movie'].cpu().numpy()\n",
    "\n",
    "print(f\"영화 임베딩 shape: {movie_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movieId 순서 복원\n",
    "idx_to_movie = {v: k for k, v in movie_to_idx.items()}\n",
    "movie_ids = np.array([idx_to_movie[i] for i in range(len(movie_to_idx))])\n",
    "\n",
    "# Parquet으로 저장\n",
    "EMBEDDINGS_DIR = PROCESSED.DIR / \"ablation_embeddings\"\n",
    "EMBEDDINGS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "df_emb = pd.DataFrame({\n",
    "    'movieId': movie_ids,\n",
    "    'embedding': [e.tolist() for e in movie_embeddings]\n",
    "})\n",
    "\n",
    "save_path = EMBEDDINGS_DIR / \"emb_kg_gnn_bert.parquet\"\n",
    "df_emb.to_parquet(save_path, index=False)\n",
    "\n",
    "print(f\"저장 완료: {save_path}\")\n",
    "print(f\"  영화 수: {len(df_emb):,}\")\n",
    "print(f\"  파일 크기: {save_path.stat().st_size / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 임베딩 품질 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embedding_stats(emb):\n",
    "    \"\"\"임베딩 품질 통계\"\"\"\n",
    "    norms = np.linalg.norm(emb, axis=1)\n",
    "    valid = norms > 1e-10\n",
    "    emb_norm = emb[valid] / norms[valid, None]\n",
    "    \n",
    "    # Anisotropy\n",
    "    anisotropy = float(np.linalg.norm(emb_norm.mean(axis=0)))\n",
    "    \n",
    "    # Random pair cosine\n",
    "    rng = np.random.default_rng(42)\n",
    "    n = len(emb_norm)\n",
    "    idx_a = rng.integers(0, n, size=50000)\n",
    "    idx_b = rng.integers(0, n, size=50000)\n",
    "    idx_b[idx_a == idx_b] = (idx_b[idx_a == idx_b] + 1) % n\n",
    "    pair_cos = np.sum(emb_norm[idx_a] * emb_norm[idx_b], axis=1)\n",
    "    \n",
    "    return {\n",
    "        'anisotropy': anisotropy,\n",
    "        'pair_cos_mean': float(np.mean(pair_cos)),\n",
    "        'pair_cos_std': float(np.std(pair_cos)),\n",
    "        'pair_cos_p5': float(np.percentile(pair_cos, 5)),\n",
    "        'pair_cos_p95': float(np.percentile(pair_cos, 95)),\n",
    "    }\n",
    "\n",
    "stats = compute_embedding_stats(movie_embeddings)\n",
    "\n",
    "print(\"[KG GNN + BERT 임베딩 품질]\")\n",
    "print(f\"  Anisotropy:      {stats['anisotropy']:.4f}\")\n",
    "print(f\"  Pair Cos Mean:   {stats['pair_cos_mean']:.4f}\")\n",
    "print(f\"  Pair Cos Std:    {stats['pair_cos_std']:.4f}\")\n",
    "print(f\"  Pair Cos p5:     {stats['pair_cos_p5']:.4f}\")\n",
    "print(f\"  Pair Cos p95:    {stats['pair_cos_p95']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다른 버전과 비교\n",
    "print(\"\\n[버전별 비교]\")\n",
    "print(\"=\"*65)\n",
    "print(f\"{'지표':<20} {'KG GNN':>15} {'KG GNN+BERT':>15}\")\n",
    "print(\"-\"*65)\n",
    "print(f\"{'Anisotropy':<20} {'~0.70':>15} {stats['anisotropy']:>15.4f}\")\n",
    "print(f\"{'Pair Cos Mean':<20} {'~0.49':>15} {stats['pair_cos_mean']:>15.4f}\")\n",
    "print(f\"{'Pair Cos p95':<20} {'~0.94':>15} {stats['pair_cos_p95']:>15.4f}\")\n",
    "print(\"=\"*65)\n",
    "print(\"\\n→ BERT 텍스트 정보 추가로 영화 콘텐츠 기반 유사도 반영\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
