{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer 기반 시퀀스 추천 모델\n",
    "\n",
    "유저의 영화 시청 시퀀스를 기반으로 다음에 볼 영화를 예측하는 모델\n",
    "\n",
    "## 모델 구조\n",
    "```\n",
    "Input: [movie_1, movie_2, ..., movie_t] (시청 순서대로)\n",
    "       ↓ (KG+BERT embedding lookup)\n",
    "       [emb_1, emb_2, ..., emb_t] (300-dim each)\n",
    "       ↓ (+ positional encoding)\n",
    "       Transformer Encoder\n",
    "       ↓\n",
    "       predicted_next_embedding (300-dim)\n",
    "       ↓ (cosine similarity with all movie embeddings)\n",
    "       Top-K 추천\n",
    "```\n",
    "\n",
    "## 학습 방식\n",
    "- Input: 유저의 시청 시퀀스 [1:t]\n",
    "- Target: 다음 영화 임베딩 [t+1]\n",
    "- Loss: Cosine Embedding Loss 또는 InfoNCE (Contrastive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# MPS fallback 설정 (Mac에서 Transformer 호환성 문제 해결)\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "sys.path.insert(0, \"/Users/jisoo/projects/thesis/carte_test\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "\n",
    "from config import PROCESSED\n",
    "\n",
    "# 한글 폰트\n",
    "if platform.system() == 'Darwin':\n",
    "    plt.rcParams['font.family'] = 'AppleGothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# Device\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평점 수: 13,717,662\n",
      "유저 수: 200,948\n",
      "영화 수: 54,520\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3248</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1084486164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1957</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1084486061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>534</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1084486058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2150</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1084486055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1084486051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   timestamp\n",
       "0       3     3248     4.0  1084486164\n",
       "1       3     1957     5.0  1084486061\n",
       "2       3      534     4.0  1084486058\n",
       "3       3     2150     4.0  1084486055\n",
       "4       3       26     4.0  1084486051"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평점 데이터 로드\n",
    "ratings = pd.read_parquet(PROCESSED.RATINGS_PARQUET)\n",
    "print(f\"평점 수: {len(ratings):,}\")\n",
    "print(f\"유저 수: {ratings['userId'].nunique():,}\")\n",
    "print(f\"영화 수: {ratings['movieId'].nunique():,}\")\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 영화 수: 53,630\n",
      "임베딩 차원: 300\n"
     ]
    }
   ],
   "source": [
    "# KG+BERT 임베딩 로드\n",
    "EMB_PATH = PROCESSED.DIR / \"ablation_embeddings\" / \"emb_kg_gnn_bert.parquet\"\n",
    "\n",
    "if not EMB_PATH.exists():\n",
    "    print(f\"Warning: {EMB_PATH} not found, trying kg_gnn...\")\n",
    "    EMB_PATH = PROCESSED.DIR / \"ablation_embeddings\" / \"emb_kg_gnn.parquet\"\n",
    "\n",
    "emb_df = pd.read_parquet(EMB_PATH)\n",
    "print(f\"임베딩 영화 수: {len(emb_df):,}\")\n",
    "\n",
    "# 임베딩 행렬 생성\n",
    "movie_ids = emb_df['movieId'].to_numpy()\n",
    "embeddings = np.array(emb_df['embedding'].tolist(), dtype=np.float32)\n",
    "emb_dim = embeddings.shape[1]\n",
    "print(f\"임베딩 차원: {emb_dim}\")\n",
    "\n",
    "# movieId → index 매핑\n",
    "movie_to_idx = {mid: i for i, mid in enumerate(movie_ids)}\n",
    "idx_to_movie = {i: mid for mid, i in movie_to_idx.items()}\n",
    "\n",
    "# 정규화\n",
    "norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "embeddings_norm = embeddings / (norms + 1e-12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 시퀀스 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩이 있는 영화만 필터링\n",
    "valid_movies = set(movie_ids)\n",
    "ratings_filtered = ratings[ratings['movieId'].isin(valid_movies)].copy()\n",
    "print(f\"필터링 후 평점 수: {len(ratings_filtered):,}\")\n",
    "\n",
    "# ========================================\n",
    "# 학습 데이터 설정\n",
    "# ========================================\n",
    "SAMPLE_USERS = 10000  # 유저 수 (전체: None)\n",
    "\n",
    "if SAMPLE_USERS is not None:\n",
    "    sampled_user_ids = ratings_filtered['userId'].drop_duplicates().sample(n=SAMPLE_USERS, random_state=42)\n",
    "    ratings_filtered = ratings_filtered[ratings_filtered['userId'].isin(sampled_user_ids)]\n",
    "    print(f\"샘플링 후 평점 수: {len(ratings_filtered):,} ({SAMPLE_USERS:,} 유저)\")\n",
    "\n",
    "# 유저별로 시청 시퀀스 생성 (timestamp 순서)\n",
    "print(\"유저별 시퀀스 생성 중...\")\n",
    "user_sequences = (\n",
    "    ratings_filtered\n",
    "    .sort_values(['userId', 'timestamp'])\n",
    "    .groupby('userId')['movieId']\n",
    "    .apply(list)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# 시퀀스 길이 통계\n",
    "seq_lengths = [len(seq) for seq in user_sequences.values()]\n",
    "print(f\"\\n유저 수: {len(user_sequences):,}\")\n",
    "print(f\"시퀀스 길이 - min: {min(seq_lengths)}, max: {max(seq_lengths)}, mean: {np.mean(seq_lengths):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Val/Test 분할 (유저 단위)\n",
    "# 각 유저의 마지막 영화를 test, 그 전 영화를 val로 사용\n",
    "\n",
    "MIN_SEQ_LEN = 5  # 최소 시퀀스 길이\n",
    "MAX_SEQ_LEN = 50  # 최대 시퀀스 길이 (Transformer 입력)\n",
    "MAX_TRAIN_PER_USER = 50  # 유저당 최대 train 샘플\n",
    "\n",
    "train_data = []\n",
    "val_data = []\n",
    "test_data = []\n",
    "\n",
    "for user_id, seq in user_sequences.items():\n",
    "    if len(seq) < MIN_SEQ_LEN:\n",
    "        continue\n",
    "    \n",
    "    # Test: 마지막 영화 예측\n",
    "    test_seq = seq[:-1][-MAX_SEQ_LEN:]\n",
    "    test_target = seq[-1]\n",
    "    test_data.append((user_id, test_seq, test_target))\n",
    "    \n",
    "    # Val: 마지막-1 영화 예측\n",
    "    if len(seq) >= MIN_SEQ_LEN + 1:\n",
    "        val_seq = seq[:-2][-MAX_SEQ_LEN:]\n",
    "        val_target = seq[-2]\n",
    "        val_data.append((user_id, val_seq, val_target))\n",
    "    \n",
    "    # Train: 유저당 최대 MAX_TRAIN_PER_USER개만 샘플링\n",
    "    train_positions = list(range(MIN_SEQ_LEN - 1, len(seq) - 2))\n",
    "    if len(train_positions) > MAX_TRAIN_PER_USER:\n",
    "        # 균등하게 샘플링\n",
    "        step = len(train_positions) // MAX_TRAIN_PER_USER\n",
    "        train_positions = train_positions[::step][:MAX_TRAIN_PER_USER]\n",
    "    \n",
    "    for i in train_positions:\n",
    "        train_seq = seq[max(0, i - MAX_SEQ_LEN + 1):i + 1]\n",
    "        train_target = seq[i + 1]\n",
    "        train_data.append((user_id, train_seq, train_target))\n",
    "\n",
    "print(f\"Train samples: {len(train_data):,}\")\n",
    "print(f\"Val samples: {len(val_data):,}\")\n",
    "print(f\"Test samples: {len(test_data):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data, movie_to_idx, embeddings, max_len=50):\n",
    "        \"\"\"\n",
    "        data: list of (user_id, sequence, target_movie_id)\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.movie_to_idx = movie_to_idx\n",
    "        self.embeddings = torch.from_numpy(embeddings)\n",
    "        self.max_len = max_len\n",
    "        self.emb_dim = embeddings.shape[1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        user_id, seq, target = self.data[idx]\n",
    "        \n",
    "        # 시퀀스를 인덱스로 변환\n",
    "        seq_idx = [self.movie_to_idx[mid] for mid in seq]\n",
    "        target_idx = self.movie_to_idx[target]\n",
    "        \n",
    "        # 시퀀스 임베딩\n",
    "        seq_emb = self.embeddings[seq_idx]  # (seq_len, emb_dim)\n",
    "        target_emb = self.embeddings[target_idx]  # (emb_dim,)\n",
    "        \n",
    "        # 패딩 (앞에서부터)\n",
    "        seq_len = len(seq_idx)\n",
    "        if seq_len < self.max_len:\n",
    "            pad = torch.zeros(self.max_len - seq_len, self.emb_dim)\n",
    "            seq_emb = torch.cat([pad, seq_emb], dim=0)\n",
    "            mask = torch.cat([torch.zeros(self.max_len - seq_len), torch.ones(seq_len)])\n",
    "        else:\n",
    "            seq_emb = seq_emb[-self.max_len:]\n",
    "            mask = torch.ones(self.max_len)\n",
    "        \n",
    "        return {\n",
    "            'seq_emb': seq_emb,  # (max_len, emb_dim)\n",
    "            'mask': mask,  # (max_len,)\n",
    "            'target_emb': target_emb,  # (emb_dim,)\n",
    "            'target_idx': target_idx,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 79\n",
      "Val batches: 8\n",
      "Test batches: 8\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 생성\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "train_dataset = SeqDataset(train_data, movie_to_idx, embeddings_norm, MAX_SEQ_LEN)\n",
    "val_dataset = SeqDataset(val_data, movie_to_idx, embeddings_norm, MAX_SEQ_LEN)\n",
    "test_dataset = SeqDataset(test_data, movie_to_idx, embeddings_norm, MAX_SEQ_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transformer 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=100):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # (1, max_len, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        return x + self.pe[:, :x.size(1), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqRecommender(nn.Module):\n",
    "    def __init__(self, emb_dim=300, n_heads=6, n_layers=2, dim_feedforward=512, dropout=0.1, max_len=50):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "        \n",
    "        # Input projection (optional, 임베딩 차원 조정)\n",
    "        self.input_proj = nn.Linear(emb_dim, emb_dim)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(emb_dim, max_len)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=emb_dim,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            nn.LayerNorm(emb_dim),\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, seq_emb, mask=None):\n",
    "        \"\"\"\n",
    "        seq_emb: (batch, seq_len, emb_dim)\n",
    "        mask: (batch, seq_len) - 1 for valid, 0 for padding\n",
    "        \n",
    "        Returns: (batch, emb_dim) - predicted next embedding\n",
    "        \"\"\"\n",
    "        # Input projection\n",
    "        x = self.input_proj(seq_emb)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Positional encoding\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        # Create attention mask (True = ignore)\n",
    "        if mask is not None:\n",
    "            attn_mask = (mask == 0)  # (batch, seq_len)\n",
    "        else:\n",
    "            attn_mask = None\n",
    "        \n",
    "        # Transformer\n",
    "        x = self.transformer(x, src_key_padding_mask=attn_mask)  # (batch, seq_len, emb_dim)\n",
    "        \n",
    "        # 마지막 유효한 위치의 출력 추출\n",
    "        if mask is not None:\n",
    "            # 각 배치에서 마지막 유효 위치 찾기\n",
    "            seq_lens = mask.sum(dim=1).long()  # (batch,)\n",
    "            batch_idx = torch.arange(x.size(0), device=x.device)\n",
    "            last_idx = seq_lens - 1\n",
    "            x = x[batch_idx, last_idx]  # (batch, emb_dim)\n",
    "        else:\n",
    "            x = x[:, -1, :]  # (batch, emb_dim)\n",
    "        \n",
    "        # Output projection\n",
    "        x = self.output_proj(x)\n",
    "        \n",
    "        # L2 정규화 (cosine similarity를 위해)\n",
    "        x = F.normalize(x, p=2, dim=-1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 1,522,024\n",
      "SeqRecommender(\n",
      "  (input_proj): Linear(in_features=300, out_features=300, bias=True)\n",
      "  (pos_encoder): PositionalEncoding()\n",
      "  (transformer): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=300, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=300, bias=True)\n",
      "        (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_proj): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "model = SeqRecommender(\n",
    "    emb_dim=emb_dim,\n",
    "    n_heads=6,\n",
    "    n_layers=2,\n",
    "    dim_feedforward=512,\n",
    "    dropout=0.1,\n",
    "    max_len=MAX_SEQ_LEN,\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfoNCELoss(nn.Module):\n",
    "    \"\"\"Contrastive loss with in-batch negatives\"\"\"\n",
    "    def __init__(self, temperature=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def forward(self, pred_emb, target_emb):\n",
    "        \"\"\"\n",
    "        pred_emb: (batch, emb_dim) - normalized\n",
    "        target_emb: (batch, emb_dim) - normalized\n",
    "        \"\"\"\n",
    "        # Similarity matrix (batch x batch)\n",
    "        sim = torch.mm(pred_emb, target_emb.t()) / self.temperature  # (batch, batch)\n",
    "        \n",
    "        # Labels: diagonal (positive pairs)\n",
    "        labels = torch.arange(sim.size(0), device=sim.device)\n",
    "        \n",
    "        # Cross entropy loss\n",
    "        loss = F.cross_entropy(sim, labels)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss & Optimizer\n",
    "criterion = InfoNCELoss(temperature=0.07)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)  # T_max = N_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, embeddings_norm, device, k_list=[10, 20, 50]):\n",
    "    \"\"\"Hit@K, MRR 계산\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_emb = torch.from_numpy(embeddings_norm).to(device)  # (n_movies, emb_dim)\n",
    "    \n",
    "    hits = {k: 0 for k in k_list}\n",
    "    mrr_sum = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            seq_emb = batch['seq_emb'].to(device)\n",
    "            mask = batch['mask'].to(device)\n",
    "            target_idx = batch['target_idx'].to(device)\n",
    "            \n",
    "            # 예측\n",
    "            pred_emb = model(seq_emb, mask)  # (batch, emb_dim)\n",
    "            \n",
    "            # 모든 영화와의 유사도\n",
    "            scores = torch.mm(pred_emb, all_emb.t())  # (batch, n_movies)\n",
    "            \n",
    "            # 랭킹\n",
    "            _, indices = scores.sort(descending=True, dim=1)\n",
    "            \n",
    "            for i in range(len(target_idx)):\n",
    "                target = target_idx[i].item()\n",
    "                rank = (indices[i] == target).nonzero(as_tuple=True)[0].item() + 1\n",
    "                \n",
    "                for k in k_list:\n",
    "                    if rank <= k:\n",
    "                        hits[k] += 1\n",
    "                \n",
    "                mrr_sum += 1.0 / rank\n",
    "                total += 1\n",
    "    \n",
    "    results = {f'Hit@{k}': hits[k] / total for k in k_list}\n",
    "    results['MRR'] = mrr_sum / total\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 루프\n",
    "N_EPOCHS = 10  # 에폭 수 증가\n",
    "best_mrr = 0\n",
    "history = {'train_loss': [], 'val_loss': [], 'val_mrr': [], 'val_hit10': []}\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{N_EPOCHS}')\n",
    "    for batch in pbar:\n",
    "        seq_emb = batch['seq_emb'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "        target_emb = batch['target_emb'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred_emb = model(seq_emb, mask)\n",
    "        loss = criterion(pred_emb, target_emb)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        pbar.set_postfix(loss=f'{loss.item():.4f}')\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation Loss 계산\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            seq_emb = batch['seq_emb'].to(device)\n",
    "            mask = batch['mask'].to(device)\n",
    "            target_emb = batch['target_emb'].to(device)\n",
    "            \n",
    "            pred_emb = model(seq_emb, mask)\n",
    "            loss = criterion(pred_emb, target_emb)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    # Validation Metrics\n",
    "    val_results = evaluate(model, val_loader, embeddings_norm, device)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_mrr'].append(val_results['MRR'])\n",
    "    history['val_hit10'].append(val_results['Hit@10'])\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, \"\n",
    "          f\"Val MRR={val_results['MRR']:.4f}, Hit@10={val_results['Hit@10']:.4f}\")\n",
    "    \n",
    "    # Best model 저장\n",
    "    if val_results['MRR'] > best_mrr:\n",
    "        best_mrr = val_results['MRR']\n",
    "        torch.save(model.state_dict(), PROCESSED.DIR / 'seq_recommender_best.pt')\n",
    "        print(f\"  -> Best model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 곡선 시각화\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# 1. Loss (Train & Validation)\n",
    "axes[0].plot(range(1, len(history['train_loss'])+1), history['train_loss'], marker='o', label='Train Loss', color='blue')\n",
    "axes[0].plot(range(1, len(history['val_loss'])+1), history['val_loss'], marker='s', label='Val Loss', color='red')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training & Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. MRR\n",
    "axes[1].plot(range(1, len(history['val_mrr'])+1), history['val_mrr'], marker='o', color='green')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MRR')\n",
    "axes[1].set_title('Validation MRR')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Hit@10\n",
    "axes[2].plot(range(1, len(history['val_hit10'])+1), history['val_hit10'], marker='s', color='purple')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Hit@10')\n",
    "axes[2].set_title('Validation Hit@10')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 최종 결과 출력\n",
    "print(\"\\n[학습 결과 요약]\")\n",
    "print(f\"  최종 Train Loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"  최종 Val Loss:   {history['val_loss'][-1]:.4f}\")\n",
    "print(f\"  최종 Val MRR:    {history['val_mrr'][-1]:.4f}\")\n",
    "print(f\"  최종 Val Hit@10: {history['val_hit10'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 테스트 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model 로드\n",
    "model.load_state_dict(torch.load(PROCESSED.DIR / 'seq_recommender_best.pt', weights_only=True))\n",
    "\n",
    "# Test 평가\n",
    "test_results = evaluate(model, test_loader, embeddings_norm, device, k_list=[1, 5, 10, 20, 50, 100])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Test Results\")\n",
    "print(\"=\"*50)\n",
    "for metric, value in test_results.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Baseline 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_baseline(loader, embeddings_norm, device, method='mean'):\n",
    "    \"\"\"Baseline: 시퀀스 임베딩의 단순 평균/마지막\"\"\"\n",
    "    all_emb = torch.from_numpy(embeddings_norm).to(device)\n",
    "    \n",
    "    hits = {k: 0 for k in [1, 5, 10, 20, 50, 100]}\n",
    "    mrr_sum = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            seq_emb = batch['seq_emb'].to(device)  # (batch, seq_len, emb_dim)\n",
    "            mask = batch['mask'].to(device)\n",
    "            target_idx = batch['target_idx'].to(device)\n",
    "            \n",
    "            if method == 'mean':\n",
    "                # Masked mean\n",
    "                mask_expanded = mask.unsqueeze(-1)  # (batch, seq_len, 1)\n",
    "                pred_emb = (seq_emb * mask_expanded).sum(dim=1) / mask.sum(dim=1, keepdim=True)\n",
    "            elif method == 'last':\n",
    "                # 마지막 유효 위치\n",
    "                seq_lens = mask.sum(dim=1).long()\n",
    "                batch_idx = torch.arange(seq_emb.size(0), device=device)\n",
    "                pred_emb = seq_emb[batch_idx, seq_lens - 1]\n",
    "            \n",
    "            pred_emb = F.normalize(pred_emb, p=2, dim=-1)\n",
    "            \n",
    "            scores = torch.mm(pred_emb, all_emb.t())\n",
    "            _, indices = scores.sort(descending=True, dim=1)\n",
    "            \n",
    "            for i in range(len(target_idx)):\n",
    "                target = target_idx[i].item()\n",
    "                rank = (indices[i] == target).nonzero(as_tuple=True)[0].item() + 1\n",
    "                \n",
    "                for k in hits:\n",
    "                    if rank <= k:\n",
    "                        hits[k] += 1\n",
    "                mrr_sum += 1.0 / rank\n",
    "                total += 1\n",
    "    \n",
    "    results = {f'Hit@{k}': hits[k] / total for k in hits}\n",
    "    results['MRR'] = mrr_sum / total\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline 평가\n",
    "print(\"Baseline 평가 중...\")\n",
    "baseline_mean = evaluate_baseline(test_loader, embeddings_norm, device, method='mean')\n",
    "baseline_last = evaluate_baseline(test_loader, embeddings_norm, device, method='last')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Baseline vs Transformer 비교\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Method':<20} {'MRR':>10} {'Hit@10':>10} {'Hit@20':>10} {'Hit@50':>10}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Mean Pooling':<20} {baseline_mean['MRR']:>10.4f} {baseline_mean['Hit@10']:>10.4f} {baseline_mean['Hit@20']:>10.4f} {baseline_mean['Hit@50']:>10.4f}\")\n",
    "print(f\"{'Last Item':<20} {baseline_last['MRR']:>10.4f} {baseline_last['Hit@10']:>10.4f} {baseline_last['Hit@20']:>10.4f} {baseline_last['Hit@50']:>10.4f}\")\n",
    "print(f\"{'Transformer':<20} {test_results['MRR']:>10.4f} {test_results['Hit@10']:>10.4f} {test_results['Hit@20']:>10.4f} {test_results['Hit@50']:>10.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비교 시각화\n",
    "methods = ['Mean Pooling', 'Last Item', 'Transformer']\n",
    "metrics = ['MRR', 'Hit@10', 'Hit@20', 'Hit@50']\n",
    "data = {\n",
    "    'Mean Pooling': baseline_mean,\n",
    "    'Last Item': baseline_last,\n",
    "    'Transformer': test_results,\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    vals = [data[method][m] for m in metrics]\n",
    "    bars = ax.bar(x + i*width, vals, width, label=method)\n",
    "    for bar, val in zip(bars, vals):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "                f'{val:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('추천 성능 비교: Baseline vs Transformer')\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim(0, max(max(data[m][metric] for m in methods for metric in metrics) * 1.2, 0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 추천 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카탈로그 로드 (영화 제목 및 피처 정보용)\n",
    "catalog = pd.read_parquet(PROCESSED.MOVIE_CATALOG_PARQUET)\n",
    "movie_titles = catalog.set_index('movieId')['original_title'].to_dict()\n",
    "\n",
    "# 번역 라이브러리 로드 시도\n",
    "try:\n",
    "    from deep_translator import GoogleTranslator\n",
    "    translator = GoogleTranslator(source='en', target='ko')\n",
    "    HAS_TRANSLATOR = True\n",
    "    print(\"번역 기능 활성화 (deep_translator)\")\n",
    "except ImportError:\n",
    "    try:\n",
    "        from googletrans import Translator\n",
    "        translator = Translator()\n",
    "        HAS_TRANSLATOR = True\n",
    "        print(\"번역 기능 활성화 (googletrans)\")\n",
    "    except ImportError:\n",
    "        HAS_TRANSLATOR = False\n",
    "        print(\"번역 라이브러리 없음 - pip install deep-translator 또는 pip install googletrans==4.0.0-rc1\")\n",
    "\n",
    "def translate_text(text):\n",
    "    \"\"\"텍스트를 한국어로 번역\"\"\"\n",
    "    if not HAS_TRANSLATOR or pd.isna(text) or text == '':\n",
    "        return None\n",
    "    try:\n",
    "        if hasattr(translator, 'translate') and callable(getattr(translator, 'translate')):\n",
    "            result = translator.translate(text)\n",
    "            # googletrans는 객체 반환, deep_translator는 문자열 반환\n",
    "            return result.text if hasattr(result, 'text') else result\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "# 피처 컬럼 정의\n",
    "FEATURE_COLS = ['original_title', 'release_year', 'genre_1', 'genre_2', 'director_1', \n",
    "                'actor_1', 'actor_2', 'produced_by_company_1', 'tagline', 'overview']\n",
    "\n",
    "def get_movie_features(movie_id):\n",
    "    \"\"\"영화의 주요 피처 정보 반환\"\"\"\n",
    "    row = catalog[catalog['movieId'] == movie_id]\n",
    "    if len(row) == 0:\n",
    "        return None\n",
    "    return row[FEATURE_COLS].iloc[0].to_dict()\n",
    "\n",
    "def print_movie_features(movie_id, prefix=\"\", show_translation=True):\n",
    "    \"\"\"영화 피처 정보 출력 (tagline, overview 포함)\"\"\"\n",
    "    features = get_movie_features(movie_id)\n",
    "    if features is None:\n",
    "        print(f\"{prefix}영화 정보 없음 (ID: {movie_id})\")\n",
    "        return\n",
    "    \n",
    "    print(f\"{prefix}{features['original_title']} ({features['release_year']})\")\n",
    "    print(f\"{prefix}  장르: {features['genre_1']}, {features['genre_2']}\")\n",
    "    print(f\"{prefix}  감독: {features['director_1']}\")\n",
    "    print(f\"{prefix}  배우: {features['actor_1']}, {features['actor_2']}\")\n",
    "    print(f\"{prefix}  제작사: {features['produced_by_company_1']}\")\n",
    "    \n",
    "    # Tagline\n",
    "    tagline = features.get('tagline', '')\n",
    "    if pd.notna(tagline) and tagline:\n",
    "        print(f\"{prefix}  태그라인: {tagline}\")\n",
    "        if show_translation and HAS_TRANSLATOR:\n",
    "            tagline_ko = translate_text(tagline)\n",
    "            if tagline_ko:\n",
    "                print(f\"{prefix}  태그라인(한국어): {tagline_ko}\")\n",
    "    \n",
    "    # Overview (길면 200자로 자름)\n",
    "    overview = features.get('overview', '')\n",
    "    if pd.notna(overview) and overview:\n",
    "        overview_short = overview[:200] + \"...\" if len(overview) > 200 else overview\n",
    "        print(f\"{prefix}  줄거리: {overview_short}\")\n",
    "        if show_translation and HAS_TRANSLATOR:\n",
    "            overview_ko = translate_text(overview_short)\n",
    "            if overview_ko:\n",
    "                print(f\"{prefix}  줄거리(한국어): {overview_ko}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_for_user(model, user_seq, embeddings_norm, movie_to_idx, idx_to_movie, movie_titles, top_k=10, device='cpu'):\n",
    "    \"\"\"유저 시퀀스 기반 추천\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 시퀀스를 인덱스로 변환\n",
    "    seq_idx = [movie_to_idx[mid] for mid in user_seq if mid in movie_to_idx]\n",
    "    if len(seq_idx) == 0:\n",
    "        return []\n",
    "    \n",
    "    # 임베딩\n",
    "    seq_emb = torch.from_numpy(embeddings_norm[seq_idx]).unsqueeze(0).to(device)  # (1, seq_len, emb_dim)\n",
    "    mask = torch.ones(1, len(seq_idx)).to(device)\n",
    "    \n",
    "    # 예측\n",
    "    with torch.no_grad():\n",
    "        pred_emb = model(seq_emb, mask)  # (1, emb_dim)\n",
    "    \n",
    "    # 모든 영화와 유사도\n",
    "    all_emb = torch.from_numpy(embeddings_norm).to(device)\n",
    "    scores = torch.mm(pred_emb, all_emb.t()).squeeze()  # (n_movies,)\n",
    "    \n",
    "    # 이미 본 영화 제외\n",
    "    seen_idx = set(seq_idx)\n",
    "    for idx in seen_idx:\n",
    "        scores[idx] = -float('inf')\n",
    "    \n",
    "    # Top-K\n",
    "    _, top_indices = scores.topk(top_k)\n",
    "    \n",
    "    recommendations = []\n",
    "    for idx in top_indices.cpu().numpy():\n",
    "        mid = idx_to_movie[idx]\n",
    "        title = movie_titles.get(mid, f'Unknown ({mid})')\n",
    "        score = scores[idx].item()\n",
    "        recommendations.append({'movieId': mid, 'title': title, 'score': score})\n",
    "    \n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 유저 예시 - 피처 정보 포함\n",
    "test_user_id, test_seq, test_target = test_data[0]\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(f\"유저 {test_user_id}의 시청 이력 (최근 5개)\")\n",
    "print(\"=\"*100)\n",
    "for i, mid in enumerate(test_seq[-5:]):\n",
    "    print(f\"\\n[{i+1}]\", end=\" \")\n",
    "    print_movie_features(mid)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"실제 다음 영화 (정답)\")\n",
    "print(\"=\"*100)\n",
    "print_movie_features(test_target)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"모델 추천 Top-10\")\n",
    "print(\"=\"*100)\n",
    "recs = recommend_for_user(model, test_seq, embeddings_norm, movie_to_idx, idx_to_movie, movie_titles, top_k=10, device=device)\n",
    "for i, rec in enumerate(recs):\n",
    "    marker = \"⭐ HIT!\" if rec['movieId'] == test_target else \"\"\n",
    "    print(f\"\\n[{i+1}] (score={rec['score']:.4f}) {marker}\")\n",
    "    print_movie_features(rec['movieId'], prefix=\"  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추천 영화와 시청 영화 간 코사인 유사도 분석\n",
    "print(\"=\"*100)\n",
    "print(\"추천 Top-10과 시청 이력 간 코사인 유사도\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# 시청한 영화 임베딩\n",
    "watched_ids = test_seq[-10:]  # 최근 10개\n",
    "watched_idx = [movie_to_idx[mid] for mid in watched_ids if mid in movie_to_idx]\n",
    "watched_emb = embeddings_norm[watched_idx]\n",
    "\n",
    "# 추천된 영화 임베딩\n",
    "rec_ids = [rec['movieId'] for rec in recs]\n",
    "rec_idx = [movie_to_idx[mid] for mid in rec_ids]\n",
    "rec_emb = embeddings_norm[rec_idx]\n",
    "\n",
    "# 유사도 행렬 계산 (추천 x 시청)\n",
    "sim_matrix = np.dot(rec_emb, watched_emb.T)  # (10, 10)\n",
    "\n",
    "# 추천 영화별 시청 영화와의 평균/최대 유사도\n",
    "print(\"\\n추천 영화별 시청 이력과의 유사도:\")\n",
    "print(\"-\"*100)\n",
    "print(f\"{'순위':<4} {'추천 영화':<35} {'평균 유사도':>12} {'최대 유사도':>12} {'가장 유사한 시청 영화':<30}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "for i, rec in enumerate(recs):\n",
    "    rec_title = movie_titles.get(rec['movieId'], 'Unknown')[:32]\n",
    "    avg_sim = sim_matrix[i].mean()\n",
    "    max_sim = sim_matrix[i].max()\n",
    "    most_similar_idx = sim_matrix[i].argmax()\n",
    "    most_similar_movie = movie_titles.get(watched_ids[most_similar_idx], 'Unknown')[:28]\n",
    "    \n",
    "    print(f\"{i+1:<4} {rec_title:<35} {avg_sim:>12.4f} {max_sim:>12.4f} {most_similar_movie:<30}\")\n",
    "\n",
    "# 전체 요약\n",
    "print(\"-\"*100)\n",
    "print(f\"{'전체 평균':<40} {sim_matrix.mean():>12.4f}\")\n",
    "\n",
    "# 히트맵 시각화\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "im = ax.imshow(sim_matrix, cmap='RdYlGn', vmin=0, vmax=1)\n",
    "\n",
    "# 축 레이블\n",
    "rec_titles = [movie_titles.get(mid, '?')[:20] for mid in rec_ids]\n",
    "watched_titles = [movie_titles.get(mid, '?')[:20] for mid in watched_ids]\n",
    "\n",
    "ax.set_xticks(range(len(watched_titles)))\n",
    "ax.set_yticks(range(len(rec_titles)))\n",
    "ax.set_xticklabels(watched_titles, rotation=45, ha='right', fontsize=9)\n",
    "ax.set_yticklabels(rec_titles, fontsize=9)\n",
    "\n",
    "ax.set_xlabel('시청한 영화 (최근 10개)', fontweight='bold')\n",
    "ax.set_ylabel('추천된 영화 (Top-10)', fontweight='bold')\n",
    "ax.set_title('추천 영화 vs 시청 영화 코사인 유사도', fontweight='bold', fontsize=12)\n",
    "\n",
    "# 값 표시\n",
    "for i in range(len(rec_titles)):\n",
    "    for j in range(len(watched_titles)):\n",
    "        val = sim_matrix[i, j]\n",
    "        color = 'white' if val < 0.5 else 'black'\n",
    "        ax.text(j, i, f'{val:.2f}', ha='center', va='center', color=color, fontsize=8)\n",
    "\n",
    "plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추천 영화와 시청 영화 간 공통 피처 분석\n",
    "print(\"=\"*100)\n",
    "print(\"추천 영화와 시청 영화 간 공통 피처 분석\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# 시청 영화의 피처 수집\n",
    "watched_features = {\n",
    "    'genres': set(),\n",
    "    'directors': set(),\n",
    "    'actors': set(),\n",
    "    'companies': set()\n",
    "}\n",
    "\n",
    "for mid in test_seq[-10:]:\n",
    "    f = get_movie_features(mid)\n",
    "    if f:\n",
    "        if pd.notna(f['genre_1']): watched_features['genres'].add(f['genre_1'])\n",
    "        if pd.notna(f.get('genre_2')): watched_features['genres'].add(f['genre_2'])\n",
    "        if pd.notna(f['director_1']): watched_features['directors'].add(f['director_1'])\n",
    "        if pd.notna(f['actor_1']): watched_features['actors'].add(f['actor_1'])\n",
    "        if pd.notna(f.get('actor_2')): watched_features['actors'].add(f['actor_2'])\n",
    "        if pd.notna(f['produced_by_company_1']): watched_features['companies'].add(f['produced_by_company_1'])\n",
    "\n",
    "print(f\"\\n시청 영화에서 추출된 피처:\")\n",
    "print(f\"  장르: {watched_features['genres']}\")\n",
    "print(f\"  감독: {watched_features['directors']}\")\n",
    "print(f\"  배우 (일부): {list(watched_features['actors'])[:5]}...\")\n",
    "print(f\"  제작사: {watched_features['companies']}\")\n",
    "\n",
    "# 각 추천 영화의 공통 피처 분석\n",
    "print(f\"\\n추천 영화별 공통 피처:\")\n",
    "print(\"-\"*100)\n",
    "print(f\"{'순위':<4} {'영화':<30} {'공통장르':<15} {'공통감독':<15} {'공통배우':<20} {'공통제작사':<15}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "for i, rec in enumerate(recs):\n",
    "    f = get_movie_features(rec['movieId'])\n",
    "    if f is None:\n",
    "        continue\n",
    "    \n",
    "    title = f['original_title'][:28]\n",
    "    \n",
    "    # 공통 피처 찾기\n",
    "    common_genres = []\n",
    "    if pd.notna(f['genre_1']) and f['genre_1'] in watched_features['genres']:\n",
    "        common_genres.append(f['genre_1'])\n",
    "    if pd.notna(f.get('genre_2')) and f.get('genre_2') in watched_features['genres']:\n",
    "        common_genres.append(f['genre_2'])\n",
    "    \n",
    "    common_director = f['director_1'] if pd.notna(f['director_1']) and f['director_1'] in watched_features['directors'] else '-'\n",
    "    \n",
    "    common_actors = []\n",
    "    if pd.notna(f['actor_1']) and f['actor_1'] in watched_features['actors']:\n",
    "        common_actors.append(f['actor_1'])\n",
    "    if pd.notna(f.get('actor_2')) and f.get('actor_2') in watched_features['actors']:\n",
    "        common_actors.append(f['actor_2'])\n",
    "    \n",
    "    common_company = f['produced_by_company_1'] if pd.notna(f['produced_by_company_1']) and f['produced_by_company_1'] in watched_features['companies'] else '-'\n",
    "    \n",
    "    genres_str = ','.join(common_genres)[:13] if common_genres else '-'\n",
    "    actors_str = ','.join(common_actors)[:18] if common_actors else '-'\n",
    "    director_str = common_director[:13] if common_director != '-' else '-'\n",
    "    company_str = common_company[:13] if common_company != '-' else '-'\n",
    "    \n",
    "    print(f\"{i+1:<4} {title:<30} {genres_str:<15} {director_str:<15} {actors_str:<20} {company_str:<15}\")\n",
    "\n",
    "print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 유저 예시\n",
    "print(\"=\"*80)\n",
    "print(\"여러 유저 추천 결과\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in [0, 100, 500, 1000]:\n",
    "    if i >= len(test_data):\n",
    "        continue\n",
    "    user_id, seq, target = test_data[i]\n",
    "    recs = recommend_for_user(model, seq, embeddings_norm, movie_to_idx, idx_to_movie, movie_titles, top_k=5, device=device)\n",
    "    \n",
    "    print(f\"\\n[유저 {user_id}]\")\n",
    "    print(f\"최근 시청: {', '.join([movie_titles.get(m, '?')[:20] for m in seq[-3:]])}\")\n",
    "    print(f\"실제 다음: {movie_titles.get(target, 'Unknown')}\")\n",
    "    print(f\"추천 Top-5:\")\n",
    "    for j, rec in enumerate(recs):\n",
    "        hit = \"*\" if rec['movieId'] == target else \" \"\n",
    "        print(f\"  {hit} {j+1}. {rec['title'][:40]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 모델 저장\n",
    "save_path = PROCESSED.DIR / 'seq_recommender_final.pt'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': {\n",
    "        'emb_dim': emb_dim,\n",
    "        'n_heads': 6,\n",
    "        'n_layers': 2,\n",
    "        'dim_feedforward': 512,\n",
    "        'max_len': MAX_SEQ_LEN,\n",
    "    },\n",
    "    'test_results': test_results,\n",
    "}, save_path)\n",
    "\n",
    "print(f\"모델 저장: {save_path}\")\n",
    "print(f\"\\n최종 Test 성능:\")\n",
    "for k, v in test_results.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
